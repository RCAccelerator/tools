"""Main module for data scraper."""
import logging
from datetime import datetime

from argparse import ArgumentParser
from data_scraper.common import constants
from data_scraper.core.scraper import JiraScraper, OSPDocScraper
from data_scraper.core.errata_scraper import ErrataScraper
from data_scraper.core.ci_logs_scraper import CILogsScraper
from data_scraper.processors.ci_logs_provider  import TestOperatorReportsProvider
from data_scraper.core.solutions_scraper import SolutionsScraper
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

def jira_scraper():
    """Entry point for command line execution."""
    parser = ArgumentParser("data_scraper")

    # Required arguments
    parser.add_argument("--jira_token", type=str, required=True)
    parser.add_argument("--database_client_url", type=str, required=True)
    parser.add_argument("--llm_server_url", type=str, required=True)
    parser.add_argument("--llm_api_key", type=str, required=True)
    parser.add_argument("--database_api_key", type=str, required=True)

    # Optional arguments
    parser.add_argument("--jira_url", type=str,
                        default=constants.DEFAULT_JIRA_URL)
    parser.add_argument("--max_results", type=int,
                        default=constants.DEFAULT_MAX_RESULTS)
    parser.add_argument("--chunk_size", type=int,
                        default=constants.DEFAULT_CHUNK_SIZE)
    parser.add_argument("--embedding_model", type=str,
                        default=constants.DEFAULT_EMBEDDING_MODEL)
    parser.add_argument("--jira_projects", nargs='+', type=str,
                        default=constants.DEFAULT_JIRA_PROJECTS)
    parser.add_argument("--db_collection_name", type=str,
                        default=constants.JIRA_COLLECTION_NAME)
    parser.add_argument("--scraper-processes", type=int,
                        default=constants.DEFAULT_NUM_SCRAPER_PROCESSES)
    parser.add_argument("--date_cutoff", type=datetime.fromisoformat,
                        default=datetime.fromisoformat(constants.DEFAULT_DATE_CUTOFF),
                        help=(
                            "No issues from before this date will be used. "
                            "Date must follow ISO format 'YYYY-MM-DD'"
                        )
    )
    parser.add_argument("--recreate_collection", action='store_true', default=False,
                        help="Recreate database collection from scratch.")
    args = parser.parse_args()

    config_args = {
        "jira_token": args.jira_token,
        "database_client_url": args.database_client_url,
        "llm_server_url": args.llm_server_url,
        "llm_api_key": args.llm_api_key,
        "database_api_key": args.database_api_key,
        "jira_url": args.jira_url,
        "max_results": args.max_results,
        "chunk_size": args.chunk_size,
        "embedding_model": args.embedding_model,
        "jira_projects": args.jira_projects,
        "db_collection_name": args.db_collection_name,
        "date_cutoff": args.date_cutoff,
        "scraper_processes": args.scraper_processes,
        "recreate_collection": args.recreate_collection,
    }

    scraper = JiraScraper(config_args)
    scraper.run()


def osp_doc_scraper():
    """Entry point for command line execution."""
    parser = ArgumentParser("osp_doc_scraper")

    # Required arguments
    parser.add_argument("--database_client_url", type=str, required=True)
    parser.add_argument("--llm_server_url", type=str, required=True)
    parser.add_argument("--llm_api_key", type=str, required=True)
    parser.add_argument("--database_api_key", type=str, required=True)
    parser.add_argument(
        "--docs_location",
        type=str,
        help="Path to plaintext OSP docs generated with get_opentsack_plaintext_docs.sh",
        required=True,
        )

    # Optional arguments
    parser.add_argument("--chunk_size", type=int,
                        default=constants.DEFAULT_CHUNK_SIZE)
    parser.add_argument("--embedding_model", type=str,
                        default=constants.DEFAULT_EMBEDDING_MODEL)
    parser.add_argument("--db_collection_name", type=str,
                        default=constants.OSP_DOCS_COLLECTION_NAME)
    parser.add_argument("--osp_version", type=str, default="18.0")
    parser.add_argument("--recreate_collection", action='store_true', default=False,
                        help="Recreate database collection from scratch.")
    parser.add_argument(
        "--rhoso_docs_path", type=str, default="",
        help="Path to downstream RHOSO docs generated by get_rhoso_plaintext_docs.sh")
    args = parser.parse_args()

    config_args = {
        "database_client_url": args.database_client_url,
        "llm_server_url": args.llm_server_url,
        "llm_api_key": args.llm_api_key,
        "database_api_key": args.database_api_key,
        "chunk_size": args.chunk_size,
        "embedding_model": args.embedding_model,
        "db_collection_name": args.db_collection_name,
        "docs_location": args.docs_location,
        "osp_version": args.osp_version,
        "recreate_collection": args.recreate_collection,
        "rhoso_docs_path": args.rhoso_docs_path,
    }

    scraper = OSPDocScraper(config_args)
    scraper.run()

def errata_scraper() -> None:
    """Entry point for command line execution."""
    parser = ArgumentParser("errata_scraper")

    # Required arguments
    parser.add_argument("--database_client_url", type=str, required=True)
    parser.add_argument("--llm_server_url", type=str, required=True)
    parser.add_argument("--llm_api_key", type=str, required=True)
    parser.add_argument("--database_api_key", type=str, required=True)
    parser.add_argument("--errata-url", type=str, required=True)
    parser.add_argument("--kerberos-username", type=str, required=True)
    parser.add_argument("--kerberos-password", type=str, required=True)

    # Optional arguments
    parser.add_argument("--jira_url", type=str,
                        default=constants.DEFAULT_JIRA_URL)
    parser.add_argument("--chunk_size", type=int,
                        default=constants.DEFAULT_CHUNK_SIZE)
    parser.add_argument("--embedding_model", type=str,
                        default=constants.DEFAULT_EMBEDDING_MODEL)
    parser.add_argument("--db_collection_name", type=str,
                        default=constants.ERRATA_COLLECTION_NAME)
    parser.add_argument("--errata-public-url", type=str,
                        default=constants.DEFAULT_ERRATA_PUBLIC_URL)
    parser.add_argument("--scraper-processes", type=int,
                        default=constants.DEFAULT_NUM_SCRAPER_PROCESSES)
    parser.add_argument("--errata-product-ids", nargs="*", type=int,
                        default=[])
    parser.add_argument("--date_cutoff", type=datetime.fromisoformat,
                        default=datetime.fromisoformat(constants.DEFAULT_DATE_CUTOFF),
                        help=(
                            "No issues from before this date will be used. "
                            "Date must follow ISO format 'YYYY-MM-DD'"
                        )
    )
    parser.add_argument("--recreate_collection", action='store_true', default=False,
                        help="Recreate database collection from scratch.")
    args = parser.parse_args()

    config_args = {
        "jira_url": args.jira_url,
        "database_client_url": args.database_client_url,
        "llm_server_url": args.llm_server_url,
        "llm_api_key": args.llm_api_key,
        "database_api_key": args.database_api_key,
        "chunk_size": args.chunk_size,
        "embedding_model": args.embedding_model,
        "db_collection_name": args.db_collection_name,
        "kerberos_username": args.kerberos_username,
        "kerberos_password": args.kerberos_password,
        "errata_product_ids": args.errata_product_ids,
        "errata_url": args.errata_url,
        "errata_public_url": args.errata_public_url,
        "default_errata_public_url": args.errata_public_url,
        "scraper_processes": args.scraper_processes,
        "date_cutoff": args.date_cutoff,
        "recreate_collection": args.recreate_collection,
    }

    scraper = ErrataScraper(config_args)
    scraper.run()


def ci_logs_scraper() -> None:
    """Entry point for command line execution."""
    parser = ArgumentParser("ci_logs_scraper")

    # Required arguments
    parser.add_argument("--database_client_url", type=str, required=True)
    parser.add_argument("--llm_server_url", type=str, required=True)
    parser.add_argument("--llm_api_key", type=str, required=True)
    parser.add_argument("--database_api_key", type=str, required=True)
    parser.add_argument("--zuul-url", type=str, required=True)

    # Optional arguments
    parser.add_argument("--chunk_size", type=int,
                        default=constants.DEFAULT_CHUNK_SIZE)
    parser.add_argument("--embedding_model", type=str,
                        default=constants.DEFAULT_EMBEDDING_MODEL)
    parser.add_argument("--db_collection_name", type=str,
                        default=constants.CI_LOGS_COLLECTION_NAME)
    parser.add_argument("--date_cutoff", type=datetime.fromisoformat,
                        default=datetime.fromisoformat(constants.DEFAULT_DATE_CUTOFF),
                        help=(
                            "No issues from before this date will be used. "
                            "Date must follow ISO format 'YYYY-MM-DD'"
                        )
    )
    parser.add_argument("--recreate_collection", type=bool, default=True,
                        help="Recreate database collection from scratch.")
    parser.add_argument("--pipelines", nargs='+', type=str,
                        default=constants.DEFAULT_ZULL_PIPELINES)
    parser.add_argument("--tenants", nargs='+', type=str,
                        default=constants.DEFAULT_ZULL_TENANTS)
    parser.add_argument("--populate_db_from_json", type=bool, default=False,
                    help="Used from Zuul jobs that create json file at the end of their runs.")
    args = parser.parse_args()

    config_args = {
        "database_client_url": args.database_client_url,
        "llm_server_url": args.llm_server_url,
        "llm_api_key": args.llm_api_key,
        "database_api_key": args.database_api_key,
        "chunk_size": args.chunk_size,
        "embedding_model": args.embedding_model,
        "db_collection_name": args.db_collection_name,
        "zuul_url": args.zuul_url,
        "date_cutoff": args.date_cutoff,
        "recreate_collection": args.recreate_collection,
        "pipelines": args.pipelines,
        "tenants": args.tenants,
        "tracebacks_json": "/tmp/tracebacks.json"
    }



    tenant = list(constants.DEFAULT_ZULL_TENANTS)[0] # just one tenenat is testsed ATM
    pipelines = list(constants.DEFAULT_ZULL_PIPELINES) # Just RHOSO pipelines are tested ATM

    if not args.populate_db_from_json:
    # Get test operator reports, save tracebacks and create json
        provider = TestOperatorReportsProvider(args.zuul_url,
                                            tenant,
                                            pipelines,
                                            config_args["tracebacks_json"])
        provider.run()

    # when json is ready, proceed with tracebacks and store them to QdrantDB
    scraper = CILogsScraper(config_args)
    scraper.run()


def solutions_scraper() -> None:
    """Entry point for command line execution."""
    parser = ArgumentParser("solutions_scraper")

    # Required arguments
    parser.add_argument("--database_client_url", type=str, required=True)
    parser.add_argument("--llm_server_url", type=str, required=True)
    parser.add_argument("--llm_api_key", type=str, required=True)
    parser.add_argument("--database_api_key", type=str, required=True)
    parser.add_argument("--solutions-token", type=str, required=True)

    # Optional arguments
    parser.add_argument("--solutions-url", type=str,
                        default=constants.DEFAULT_SOLUTIONS_PUBLIC_URL)
    parser.add_argument("--max_results", type=int,
                        default=constants.SOLUTIONS_MAX_RESULTS)
    parser.add_argument("--chunk_size", type=int,
                        default=constants.DEFAULT_CHUNK_SIZE)
    parser.add_argument("--embedding_model", type=str,
                        default=constants.DEFAULT_EMBEDDING_MODEL)
    parser.add_argument("--db_collection_name", type=str,
                        default=constants.SOLUTIONS_COLLECTION_NAME)
    parser.add_argument("--product_name", type=str,
                        default=constants.SOLUTIONS_PRODUCT_NAME)
    parser.add_argument("--recreate_collection", action='store_true', default=False,
                        help="Recreate database collection from scratch.")
    args = parser.parse_args()

    config_args = {
        "database_client_url": args.database_client_url,
        "llm_server_url": args.llm_server_url,
        "llm_api_key": args.llm_api_key,
        "database_api_key": args.database_api_key,
        "chunk_size": args.chunk_size,
        "embedding_model": args.embedding_model,
        "db_collection_name": args.db_collection_name,
        "solutions_url": args.solutions_url,
        "solutions_token": args.solutions_token,
        "product_name": args.product_name,
        "max_results": args.max_results,
        "recreate_collection": args.recreate_collection,
    }

    scraper = SolutionsScraper(config_args)
    scraper.run()
